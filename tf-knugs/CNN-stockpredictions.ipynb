{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"./Dataset\"\n",
    "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "# to implement F1 score for validation in a batch\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2\n",
    "\n",
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)     # pick one time step\n",
    "            n = (df.index == t).argmax() # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # this sample is not enough for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []\n",
    "\n",
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        n = (df.index == t).argmax()\n",
    "        # extract sample using a sliding window\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrames\n",
    "data = {}\n",
    "for filename in os.listdir(DATADIR):\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0]\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    data[name] = X\n",
    "\n",
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "n_features = 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 60, 1, 8)          664       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 29, 1, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 13, 1, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 104)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1169 (4.57 KB)\n",
      "Trainable params: 1169 (4.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 63s 610ms/step - loss: 0.4638 - acc: 0.5539 - f1macro: 0.4024 - val_loss: 0.4980 - val_acc: 0.5023 - val_f1macro: 0.3341\n",
      "Epoch 2/20\n",
      "  3/100 [..............................] - ETA: 2s - loss: 0.4894 - acc: 0.5104 - f1macro: 0.3378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 64s 644ms/step - loss: 0.4311 - acc: 0.5685 - f1macro: 0.3621 - val_loss: 0.4946 - val_acc: 0.5055 - val_f1macro: 0.3353\n",
      "Epoch 3/20\n",
      "  4/100 [>.............................] - ETA: 2s - loss: 0.4353 - acc: 0.5684 - f1macro: 0.3665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 68s 684ms/step - loss: 0.4306 - acc: 0.5752 - f1macro: 0.4111 - val_loss: 0.4856 - val_acc: 0.5180 - val_f1macro: 0.3405\n",
      "Epoch 4/20\n",
      "  5/100 [>.............................] - ETA: 3s - loss: 0.4257 - acc: 0.5828 - f1macro: 0.4630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.3869 - acc: 0.6317 - f1macro: 0.5475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 56s 569ms/step - loss: 0.3869 - acc: 0.6317 - f1macro: 0.5475 - val_loss: 0.4942 - val_acc: 0.5070 - val_f1macro: 0.3611\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3463 - acc: 0.6784 - f1macro: 0.6439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 62s 630ms/step - loss: 0.3463 - acc: 0.6784 - f1macro: 0.6439 - val_loss: 0.4682 - val_acc: 0.5383 - val_f1macro: 0.4143\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3103 - acc: 0.7124 - f1macro: 0.6877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 53s 539ms/step - loss: 0.3103 - acc: 0.7124 - f1macro: 0.6877 - val_loss: 0.5017 - val_acc: 0.4938 - val_f1macro: 0.4179\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2911 - acc: 0.7302 - f1macro: 0.7091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 64s 646ms/step - loss: 0.2911 - acc: 0.7302 - f1macro: 0.7091 - val_loss: 0.4860 - val_acc: 0.5195 - val_f1macro: 0.4608\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.2765 - acc: 0.7439 - f1macro: 0.7245 - val_loss: 0.4873 - val_acc: 0.5117 - val_f1macro: 0.4588\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 63s 641ms/step - loss: 0.2597 - acc: 0.7550 - f1macro: 0.7371 - val_loss: 0.4934 - val_acc: 0.5078 - val_f1macro: 0.4547\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 55s 557ms/step - loss: 0.2507 - acc: 0.7656 - f1macro: 0.7499 - val_loss: 0.5124 - val_acc: 0.4812 - val_f1macro: 0.4435\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2398 - acc: 0.7758 - f1macro: 0.7612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 53s 540ms/step - loss: 0.2398 - acc: 0.7758 - f1macro: 0.7612 - val_loss: 0.4658 - val_acc: 0.5383 - val_f1macro: 0.4989\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 70s 708ms/step - loss: 0.2412 - acc: 0.7727 - f1macro: 0.7590 - val_loss: 0.5205 - val_acc: 0.4680 - val_f1macro: 0.4416\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2277 - acc: 0.7863 - f1macro: 0.7727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 55s 552ms/step - loss: 0.2277 - acc: 0.7863 - f1macro: 0.7727 - val_loss: 0.4884 - val_acc: 0.5203 - val_f1macro: 0.5026\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 64s 647ms/step - loss: 0.2243 - acc: 0.7905 - f1macro: 0.7796 - val_loss: 0.4840 - val_acc: 0.5148 - val_f1macro: 0.4903\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 0.2136 - acc: 0.8005 - f1macro: 0.7888 - val_loss: 0.4774 - val_acc: 0.5234 - val_f1macro: 0.5006\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 52s 527ms/step - loss: 0.2219 - acc: 0.7916 - f1macro: 0.7803 - val_loss: 0.4892 - val_acc: 0.5156 - val_f1macro: 0.4707\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 53s 537ms/step - loss: 0.2086 - acc: 0.8045 - f1macro: 0.7950 - val_loss: 0.5004 - val_acc: 0.4930 - val_f1macro: 0.4673\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 65s 660ms/step - loss: 0.2027 - acc: 0.8078 - f1macro: 0.7961 - val_loss: 0.4656 - val_acc: 0.5406 - val_f1macro: 0.4954\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 56s 568ms/step - loss: 0.2069 - acc: 0.8059 - f1macro: 0.7961 - val_loss: 0.4618 - val_acc: 0.5336 - val_f1macro: 0.4991\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 53s 540ms/step - loss: 0.1979 - acc: 0.8133 - f1macro: 0.8016 - val_loss: 0.4950 - val_acc: 0.5008 - val_f1macro: 0.4744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd6e4fc12b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary()  # print model structure to console\n",
    "\n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]\n",
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=100, validation_steps=10, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step\n",
      "accuracy: 0.5365853658536586\n",
      "MAE: 0.4634146341463415\n",
      "F1: 0.6575342465753425\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
